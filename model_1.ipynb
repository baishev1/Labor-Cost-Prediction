{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCNwh2pyXZHr"
      },
      "outputs": [],
      "source": [
        "!pip install stanza\n",
        "!pip install transformers\n",
        "!pip install catboost\n",
        "!pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSfB9LCiW2EN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import stanza\n",
        "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from faker import Faker\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "nlp = stanza.Pipeline(lang=\"ru\", processors=\"tokenize,lemma\", use_gpu=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labor cost prediction"
      ],
      "metadata": {
        "id": "ZAVp8eORRi7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для генерации синтетических данных\n",
        "def generate_synthetic_data(num_rows=5000):\n",
        "    \"\"\"Генерирует DataFrame с такой же структурой, как в проекте, но с фальшивыми данными.\"\"\"\n",
        "    fake = Faker('ru_RU')\n",
        "    data = []\n",
        "\n",
        "    # Создаем пул поддельных, но реалистичных ID и названий, чтобы они повторялись\n",
        "    company_ids = [fake.uuid4() for _ in range(50)]\n",
        "    project_names = ['Проект ' + fake.bs().split(' ')[0].replace(',', '') for _ in range(30)]\n",
        "    department_names = ['Отдел ' + fake.job().split(' ')[0].replace(',', '') for _ in range(10)]\n",
        "    parent_uuids = [fake.uuid4() for _ in range(100)] + [np.nan] * 900 # Большинство - NaN\n",
        "\n",
        "    print(f\"Генерация {num_rows} строк синтетических данных (для защиты NDA)...\")\n",
        "\n",
        "    for _ in range(num_rows):\n",
        "        data.append({\n",
        "            'uuid': fake.uuid4(),\n",
        "            'subject': fake.sentence(nb_words=np.random.randint(3, 8)),\n",
        "            'details': fake.text(max_nb_chars=150) if np.random.rand() > 0.4 else np.nan,\n",
        "            'parent_uuid': np.random.choice(parent_uuids),\n",
        "            'importance': np.random.choice([1, 2, 0], p=[0.9, 0.08, 0.02]), # Основано на вашем EDA\n",
        "            'company_id': np.random.choice(company_ids),\n",
        "            'project_name': np.random.choice(project_names),\n",
        "            'department_name': np.random.choice(department_names),\n",
        "            'labor_costs': np.random.choice([0.0, 10.2, 30.0, 60.0, 100.2, 180.0]) + np.random.randint(-5, 5)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    # Убедимся, что labor_costs не отрицательные\n",
        "    df['labor_costs'] = df['labor_costs'].clip(lower=0)\n",
        "\n",
        "    # Заполняем NaN в категориальных\n",
        "    df['company_id'] = df['company_id'].fillna('__unknown__')\n",
        "    df['project_name'] = df['project_name'].fillna('__unknown__')\n",
        "    df['department_name'] = df['department_name'].fillna('__unknown__')\n",
        "\n",
        "    # создадим флаг дочерней задачи\n",
        "    df['is_subtask'] = df['parent_uuid'].notnull().astype(int)\n",
        "\n",
        "    # склеим текст\n",
        "    df['text'] = (df['subject'].fillna('') + ' ' + df['details'].fillna('')).str.strip()\n",
        "\n",
        "    # удалим лишние поля\n",
        "    df = df.drop(columns=['uuid', 'parent_uuid', 'subject', 'details'])\n",
        "\n",
        "    # логарифмируем таргет\n",
        "    df['labor_costs_log'] = np.log1p(df['labor_costs'])\n",
        "\n",
        "    # меняем порядок\n",
        "    cols = ['text', 'is_subtask'] + [c for c in df.columns if c not in ['text', 'is_subtask']]\n",
        "    df = df[cols]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Генерируем данные (10k строк вместо 580k, чтобы ноутбук быстро работал у рекрутера)\n",
        "df_synthetic = generate_synthetic_data(10000)\n",
        "\n",
        "print(\"Синтетические данные готовы и разделены.\")"
      ],
      "metadata": {
        "id": "tOGMwKaXwtx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fniUBCrzm5Ul"
      },
      "outputs": [],
      "source": [
        "# напишем фукнцию для анализа и обработки выбросов\n",
        "\n",
        "def analyze_and_trim_outliers(df, target_col='labor_costs', k=1.5, bins_before=100, bins_after=50, log_scale=True):\n",
        "    print(\"Изначальное количество данных:\", len(df))\n",
        "\n",
        "    Q1 = df[target_col].quantile(0.25)\n",
        "    Q3 = df[target_col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_threshold = Q1 - k * IQR\n",
        "    upper_threshold = Q3 + k * IQR\n",
        "\n",
        "    print(f\"IQR: {IQR:.2f}, Нижняя граница: {lower_threshold:.2f}, Верхняя граница: {upper_threshold:.2f}\")\n",
        "\n",
        "    outliers_count_lower = (df[target_col] < lower_threshold).sum()\n",
        "    outliers_count_upper = (df[target_col] > upper_threshold).sum()\n",
        "    total_outliers = outliers_count_lower + outliers_count_upper\n",
        "    print(f\"Будет удалено всего: {total_outliers:,} записей \"\n",
        "          f\"({(total_outliers/len(df))*100:.3f}%), останется {len(df)-total_outliers}\")\n",
        "\n",
        "    df_trimmed = df[(df[target_col] >= lower_threshold) & (df[target_col] <= upper_threshold)].copy()\n",
        "    print(\"Статистика после обрезки:\")\n",
        "    print(df_trimmed[target_col].describe())\n",
        "\n",
        "    # построим графики\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
        "\n",
        "    # до обрезки\n",
        "    axes[0].hist(df[target_col], bins=bins_before, alpha=0.7, edgecolor='black')\n",
        "    axes[0].axvline(lower_threshold, color='red', linestyle='--', label='IQR Lower')\n",
        "    axes[0].axvline(upper_threshold, color='red', linestyle='--', label='IQR Upper')\n",
        "    axes[0].set_title('Распределение ДО обрезки')\n",
        "    axes[0].set_xlabel(target_col)\n",
        "    axes[0].set_ylabel('Частота')\n",
        "    if log_scale:\n",
        "        axes[0].set_yscale('log')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # после обрезки\n",
        "    axes[1].hist(df_trimmed[target_col], bins=bins_after, alpha=0.7, color='green', edgecolor='black')\n",
        "    axes[1].set_title('Распределение ПОСЛЕ обрезки')\n",
        "    axes[1].set_xlabel(target_col)\n",
        "    axes[1].set_ylabel('Частота')\n",
        "    if log_scale:\n",
        "        axes[1].set_yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df_trimmed, lower_threshold, upper_threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJrh4ZZXnE8Z"
      },
      "outputs": [],
      "source": [
        "train_full, test_df = train_test_split(df_synthetic, test_size=0.1, random_state=42)\n",
        "train_df, val_df = train_test_split(train_full, test_size=0.1, random_state=42)\n",
        "\n",
        "train_df_trimmed, lower_threshold, upper_threshold = analyze_and_trim_outliers(train_df)\n",
        "val_df_trimmed = val_df[(val_df['labor_costs'] >= lower_threshold) & (val_df['labor_costs'] <= upper_threshold)].copy()\n",
        "test_df_trimmed = test_df[(test_df['labor_costs'] >= lower_threshold) & (test_df['labor_costs'] <= upper_threshold)].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_2L3GcgXG1l"
      },
      "outputs": [],
      "source": [
        "# подсчёт количества тасков на компанию\n",
        "company_task_count = train_df_trimmed[\"company_id\"].value_counts()\n",
        "project_task_count = train_df_trimmed[\"project_name\"].value_counts()\n",
        "\n",
        "print(\"Топ-10 компаний по количеству тасков:\")\n",
        "print(company_task_count.head(10))\n",
        "print(\"\\nТоп-10 проектов по количеству тасков:\")\n",
        "print(project_task_count.head(10))\n",
        "\n",
        "company_task_count.describe(), project_task_count.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0FqcRM266r8"
      },
      "outputs": [],
      "source": [
        "def create_aggregated_features(train_df, val_df, test_df, target_col='labor_costs', company_thresh=15, project_thresh=30):\n",
        "\n",
        "    train_features = train_df.copy()\n",
        "    val_features = val_df.copy()\n",
        "    test_features = test_df.copy()\n",
        "\n",
        "    # Список всех датасетов для обработки\n",
        "    datasets = [train_features, val_features, test_features]\n",
        "    dataset_names = ['train', 'val', 'test']\n",
        "\n",
        "\n",
        "    # -- company agg --\n",
        "    company_agg = train_df.groupby('company_id')[target_col].agg(\n",
        "        ['mean', 'median', 'std', 'count', 'min', 'max']\n",
        "    ).reset_index()\n",
        "\n",
        "    company_agg.columns = [\n",
        "        'company_id', 'company_avg_cost', 'company_median_cost',\n",
        "        'company_std_cost', 'company_task_count',\n",
        "        'company_min_cost', 'company_max_cost'\n",
        "    ]\n",
        "    company_agg['company_std_cost'] = company_agg['company_std_cost'].fillna(0)\n",
        "\n",
        "    # Применяем к всем датасетам\n",
        "    for i, df in enumerate(datasets):\n",
        "        datasets[i] = df.merge(company_agg, on='company_id', how='left')\n",
        "\n",
        "    # Обработка пропусков для всех датасетов (кроме train)\n",
        "    global_mean = train_df[target_col].mean()\n",
        "    company_cols = ['company_avg_cost', 'company_median_cost', 'company_std_cost',\n",
        "                   'company_task_count', 'company_min_cost', 'company_max_cost']\n",
        "\n",
        "    for i, df in enumerate(datasets):\n",
        "        if i == 0:  # train - пропускаем\n",
        "            continue\n",
        "        for col in company_cols:\n",
        "            if col == 'company_task_count':\n",
        "                df[col] = df[col].fillna(1)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(global_mean)\n",
        "\n",
        "    # Редкие компании\n",
        "    company_task_count = train_df['company_id'].value_counts()\n",
        "    for i, df in enumerate(datasets):\n",
        "        df['is_rare_company'] = df['company_id'].map(\n",
        "            lambda x: company_task_count.get(x, 0) < company_thresh\n",
        "        )\n",
        "\n",
        "    # -- department agg --\n",
        "    dept_agg = train_df.groupby('department_name')[target_col].agg(\n",
        "        ['mean', 'median', 'count', 'std']\n",
        "    ).reset_index()\n",
        "\n",
        "    dept_agg.columns = ['department_name', 'dept_avg_cost', 'dept_median_cost',\n",
        "                       'dept_task_count', 'dept_std_cost']\n",
        "    dept_agg['dept_std_cost'] = dept_agg['dept_std_cost'].fillna(0)\n",
        "\n",
        "    # Применяем к всем датасетам\n",
        "    for i, df in enumerate(datasets):\n",
        "        datasets[i] = df.merge(dept_agg, on='department_name', how='left')\n",
        "\n",
        "    # Обработка пропусков для департаментов\n",
        "    dept_cols = ['dept_avg_cost', 'dept_median_cost', 'dept_task_count', 'dept_std_cost']\n",
        "    for i, df in enumerate(datasets):\n",
        "        if i == 0:  # train - пропускаем\n",
        "            continue\n",
        "        for col in dept_cols:\n",
        "            if col == 'dept_task_count':\n",
        "                df[col] = df[col].fillna(1)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(global_mean)\n",
        "\n",
        "    # -- project agg --\n",
        "    project_agg = train_df.groupby('project_name')[target_col].agg(\n",
        "        ['mean', 'median', 'count', 'std']\n",
        "    ).reset_index()\n",
        "\n",
        "    project_agg.columns = ['project_name', 'project_avg_cost', 'project_median_cost',\n",
        "                          'project_task_count', 'project_std_cost']\n",
        "    project_agg['project_std_cost'] = project_agg['project_std_cost'].fillna(0)\n",
        "\n",
        "    # Применяем к всем датасетам\n",
        "    for i, df in enumerate(datasets):\n",
        "        datasets[i] = df.merge(project_agg, on='project_name', how='left')\n",
        "\n",
        "    # Обработка пропусков для проектов\n",
        "    project_cols = ['project_avg_cost', 'project_median_cost', 'project_task_count', 'project_std_cost']\n",
        "    for i, df in enumerate(datasets):\n",
        "        if i == 0:  # train - пропускаем\n",
        "            continue\n",
        "        for col in project_cols:\n",
        "            if col == 'project_task_count':\n",
        "                df[col] = df[col].fillna(1)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(global_mean)\n",
        "\n",
        "    # Редкие проекты\n",
        "    project_task_count = train_df['project_name'].value_counts()\n",
        "    for i, df in enumerate(datasets):\n",
        "        df['is_rare_project'] = df['project_name'].map(\n",
        "            lambda x: project_task_count.get(x, 0) < project_thresh\n",
        "        )\n",
        "\n",
        "    # -- interaction features --\n",
        "    for i, df in enumerate(datasets):\n",
        "        if 'importance' in df.columns:\n",
        "            df['company_avg_x_importance'] = df['company_avg_cost'] * df['importance']\n",
        "            df['dept_avg_x_importance'] = df['dept_avg_cost'] * df['importance']\n",
        "\n",
        "        # Cross-level keys\n",
        "        df['company_department'] = df['company_id'].astype(str) + \"_\" + df['department_name'].astype(str)\n",
        "        df['project_department'] = df['project_name'].astype(str) + \"_\" + df['department_name'].astype(str)\n",
        "        df['company_project'] = df['company_id'].astype(str) + \"_\" + df['project_name'].astype(str)\n",
        "\n",
        "    def add_features_to_df(df):\n",
        "        # Текстовые фичи\n",
        "        if 'text_norm' in df.columns:\n",
        "            df['num_words'] = df['text_norm'].str.split().str.len().fillna(0)\n",
        "            df['num_unique_words'] = df['text_norm'].apply(\n",
        "                lambda x: len(set(str(x).split())) if pd.notna(x) else 0\n",
        "            )\n",
        "            df['avg_word_length'] = df['text_norm'].apply(\n",
        "                lambda x: np.mean([len(word) for word in str(x).split()]) if pd.notna(x) and str(x).split() else 0\n",
        "            )\n",
        "\n",
        "        if 'text' in df.columns:\n",
        "            df['num_numbers'] = df['text'].str.count(r'\\d+').fillna(0)\n",
        "            df['has_urgent_words'] = df['text'].str.contains(\n",
        "                r'срочно|urgent|asap|немедленно|быстро|скорее', case=False, na=False\n",
        "            ).astype(int)\n",
        "            df['has_complex_words'] = df['text'].str.contains(\n",
        "                r'анализ|исследование|разработка|integration|analysis|архитектура|оптимизация',\n",
        "                case=False, na=False\n",
        "            ).astype(int)\n",
        "\n",
        "        # Плотность информации\n",
        "        if 'text_length' in df.columns and 'num_unique_words' in df.columns:\n",
        "            df['info_density'] = df['num_unique_words'] / (df['text_length'] + 1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_ratio_features(df):\n",
        "        # Соотношения с средними значениями (только для train, так как есть target)\n",
        "        if target_col in df.columns:\n",
        "            df['cost_vs_company_avg'] = df[target_col] / (df['company_avg_cost'] + 1)\n",
        "            df['cost_vs_dept_avg'] = df[target_col] / (df['dept_avg_cost'] + 1)\n",
        "            df['cost_vs_project_avg'] = df[target_col] / (df['project_avg_cost'] + 1)\n",
        "\n",
        "            # Z-scores\n",
        "            df['cost_zscore_company'] = (df[target_col] - df['company_avg_cost']) / (df['company_std_cost'] + 1)\n",
        "            df['cost_zscore_dept'] = (df[target_col] - df['dept_avg_cost']) / (df['dept_std_cost'] + 1)\n",
        "\n",
        "            # Позиция в диапазоне\n",
        "            df['company_price_range'] = df['company_max_cost'] - df['company_min_cost']\n",
        "            df['cost_position_in_range'] = (df[target_col] - df['company_min_cost']) / (df['company_price_range'] + 1)\n",
        "\n",
        "        # Коэффициенты вариации (можно для всех датасетов)\n",
        "        df['company_cv'] = df['company_std_cost'] / (df['company_avg_cost'] + 1)\n",
        "        df['dept_cv'] = df['dept_std_cost'] / (df['dept_avg_cost'] + 1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_complexity_features(df):\n",
        "        if 'importance' in df.columns and 'text_length' in df.columns:\n",
        "            df['complexity_score'] = df['text_length'] * (df['importance'] + 0.1)\n",
        "            df['text_importance_ratio'] = df['text_length'] / (df['importance'] + 0.1)\n",
        "\n",
        "            df['importance_category'] = pd.cut(\n",
        "                df['importance'],\n",
        "                bins=[-0.1, 0, 2, 5, float('inf')],\n",
        "                labels=['zero', 'low', 'medium', 'high']\n",
        "            )\n",
        "\n",
        "        if 'num_words' in df.columns and 'num_unique_words' in df.columns:\n",
        "            df['text_complexity'] = df['num_unique_words'] * df['avg_word_length']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_size_categories(df):\n",
        "        df['company_size_category'] = pd.cut(\n",
        "            df['company_task_count'],\n",
        "            bins=[0, 10, 50, 200, float('inf')],\n",
        "            labels=['small', 'medium', 'large', 'enterprise']\n",
        "        )\n",
        "\n",
        "        df['dept_size_category'] = pd.cut(\n",
        "            df['dept_task_count'],\n",
        "            bins=[0, 5, 20, 100, float('inf')],\n",
        "            labels=['tiny', 'small', 'medium', 'large']\n",
        "        )\n",
        "\n",
        "        df['project_size_category'] = pd.cut(\n",
        "            df['project_task_count'],\n",
        "            bins=[0, 3, 15, 50, float('inf')],\n",
        "            labels=['tiny', 'small', 'medium', 'large']\n",
        "        )\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Применяем все дополнительные функции\n",
        "    for i, df in enumerate(datasets):\n",
        "        datasets[i] = add_features_to_df(df)\n",
        "        datasets[i] = add_ratio_features(df)\n",
        "        datasets[i] = add_complexity_features(df)\n",
        "        datasets[i] = add_size_categories(df)\n",
        "\n",
        "        # Дополнительные комбинированные фичи\n",
        "        if 'importance' in df.columns:\n",
        "            df['company_size_x_importance'] = df['company_task_count'] * (df['importance'] + 0.1)\n",
        "            df['dept_size_x_importance'] = df['dept_task_count'] * (df['importance'] + 0.1)\n",
        "\n",
        "    # Финальная обработка пропусков\n",
        "    for i, df in enumerate(datasets):\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            if col != target_col:\n",
        "                df[col] = df[col].fillna(0)\n",
        "\n",
        "    # Обновляем ссылки\n",
        "    train_features = datasets[0]\n",
        "    val_features = datasets[1]\n",
        "    test_features = datasets[2]\n",
        "\n",
        "\n",
        "    return train_features, val_features, test_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfsyvj2oWSJl"
      },
      "outputs": [],
      "source": [
        "train, val, test = create_aggregated_features(train_df_trimmed, val_df_trimmed, test_df_trimmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTK1f17T8zad"
      },
      "outputs": [],
      "source": [
        "# создадим фукнцию для анализа корреляции численных признаков с целевой переменной\n",
        "def analyze_num_correlations(df, target_col='labor_costs_log', top_n=15):\n",
        "    # Выбираем только численные столбцы\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if target_col in numeric_cols:\n",
        "        numeric_cols.remove(target_col)\n",
        "\n",
        "    # Вычисляем корреляции\n",
        "    correlations = df[numeric_cols + [target_col]].corr()[target_col].abs().sort_values(ascending=False)[1:]\n",
        "\n",
        "    print(f\"ТОП-{top_n} признаков по корреляции с {target_col}:\")\n",
        "    print(\"=\"*50)\n",
        "    for i, (feature, corr) in enumerate(correlations.head(top_n).items(), 1):\n",
        "        print(f\"{i:2d}. {feature:<30}: {corr:.4f}\")\n",
        "\n",
        "    # Визуализация топ корреляций\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_corr = correlations.head(top_n)\n",
        "    bars = plt.barh(range(len(top_corr)), top_corr.values)\n",
        "    plt.yticks(range(len(top_corr)), top_corr.index)\n",
        "    plt.xlabel('Абсолютная корреляция')\n",
        "    plt.title(f'ТОП-{top_n} признаков по корреляции с {target_col}')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    # Добавляем значения на столбцы\n",
        "    for i, (bar, value) in enumerate(zip(bars, top_corr.values)):\n",
        "        plt.text(value + 0.01, i, f'{value:.3f}', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENxHUHc79b1-"
      },
      "outputs": [],
      "source": [
        "correlations = analyze_num_correlations(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyAQlO3w9LM7"
      },
      "outputs": [],
      "source": [
        "# создадим фукнцию для анализа корреляции категориальных признаков с целевой переменной\n",
        "def analyze_cat_correlations(df, target_col='labor_costs'):\n",
        "    categorical_cols = ['department_name', 'project_name', 'importance', 'company_id']\n",
        "    results = {}\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            print(f\"\\n--- {col.upper()} ---\")\n",
        "\n",
        "            # Статистика по категориям\n",
        "            cat_stats = df.groupby(col)[target_col].agg([\n",
        "                'count', 'mean', 'median', 'std'\n",
        "            ]).round(2)\n",
        "            cat_stats.columns = ['Count', 'Mean', 'Median', 'Std']\n",
        "            cat_stats = cat_stats.sort_values('Mean', ascending=False)\n",
        "\n",
        "            print(cat_stats)\n",
        "\n",
        "            # ANOVA для оценки значимости различий\n",
        "            groups = [group[target_col].values for name, group in df.groupby(col)]\n",
        "            f_stat, p_value = stats.f_oneway(*groups)\n",
        "\n",
        "            # Eta-squared (сила связи)\n",
        "            ss_total = ((df[target_col] - df[target_col].mean()) ** 2).sum()\n",
        "            ss_between = sum([len(group) * (np.mean(group) - df[target_col].mean()) ** 2\n",
        "                             for group in groups])\n",
        "            eta_squared = ss_between / ss_total\n",
        "\n",
        "            results[col] = {\n",
        "                'f_statistic': f_stat,\n",
        "                'p_value': p_value,\n",
        "                'eta_squared': eta_squared,\n",
        "                'significant': p_value < 0.05\n",
        "            }\n",
        "\n",
        "            print(f\"F-statistic: {f_stat:.2f}\")\n",
        "            print(f\"P-value: {p_value:.2e}\")\n",
        "            print(f\"Eta-squared: {eta_squared:.4f}\")\n",
        "            print(f\"Статистически значим: {'Да' if p_value < 0.05 else 'Нет'}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGXmkq-v9atn"
      },
      "outputs": [],
      "source": [
        "categorical_results = analyze_cat_correlations(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f23ogP_IBtMY"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2rb5xji9l_i"
      },
      "outputs": [],
      "source": [
        "def nlp_normalize(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. префиксы вида RE:, FW:, FWD:\n",
        "    text = re.sub(r\"^(re|fw|fwd)\\s*[:\\-]\\s*\", \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2. нижний регистр\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. лишние символы (оставляем буквы, цифры и пробелы)\n",
        "    text = re.sub(r\"[^a-zа-яё0-9\\s]\", \" \", text)\n",
        "\n",
        "    # 4. лемматизация\n",
        "    doc = nlp(text)\n",
        "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "    text = \" \".join(lemmas)\n",
        "\n",
        "    # 5. пробелов\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xutApJIyWtp0"
      },
      "outputs": [],
      "source": [
        "train['text_norm'] = train['text'].apply(nlp_normalize)\n",
        "val['text_norm'] = val['text'].apply(nlp_normalize)\n",
        "test['text_norm'] = test['text'].apply(nlp_normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNLU5kTUZ1XO"
      },
      "outputs": [],
      "source": [
        "train['text_length'] = train['text_norm'].apply(len)\n",
        "val['text_length'] = val['text_norm'].apply(len)\n",
        "test['text_length'] = test['text_norm'].apply(len)\n",
        "\n",
        "# основные статистики\n",
        "mean_len = train['text_length'].mean()\n",
        "max_len = train['text_length'].max()\n",
        "median_len = train['text_length'].median()\n",
        "\n",
        "print(f\"Средняя длина текста: {mean_len:.2f}\")\n",
        "print(f\"Максимальная длина текста: {max_len}\")\n",
        "print(f\"Медианная длина текста: {median_len}\")\n",
        "\n",
        "# гистограмма распределения длин текстов\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(train['text_length'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Распределение длины текстов')\n",
        "plt.xlabel('Длина текста (символы)')\n",
        "plt.ylabel('Количество текстов')\n",
        "\n",
        "# добавим линии среднего, максимума и медианы\n",
        "plt.axvline(mean_len, color='red', linestyle='--', label=f'Среднее ({mean_len:.0f})')\n",
        "plt.axvline(median_len, color='green', linestyle='-', label=f'Медиана ({median_len:.0f})')\n",
        "plt.axvline(max_len, color='purple', linestyle=':', label=f'Максимум ({max_len})')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx3u2-kVOQcJ"
      },
      "outputs": [],
      "source": [
        "model_name = \"Zamza/XLM-roberta-large-ftit-emb-lr01\"\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model.eval()  # выключаем режим обучения\n",
        "\n",
        "# Если есть GPU, переносим модель на CUDA\n",
        "model.to(device)\n",
        "\n",
        "def get_xlm_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Возвращает эмбеддинг для текста, используя XLM-R ftit-emb-lr01.\n",
        "    \"\"\"\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded)\n",
        "\n",
        "    cls_emb = outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()\n",
        "    return cls_emb\n",
        "\n",
        "def create_embeddings_and_save(df: pd.DataFrame, text_col, name_suffix) -> None:\n",
        "    \"\"\"\n",
        "    Создаёт эмбеддинги, сохраняет в .npz и гарантирует совпадение индексов.\n",
        "    \"\"\"\n",
        "    texts = df[text_col].tolist()\n",
        "    indices = df.index.to_numpy()  # Гарантированно сохраняем индексы\n",
        "    embeddings = []\n",
        "\n",
        "    # Генерация эмбеддингов с прогресс-баром\n",
        "    for text in tqdm(texts, desc=\"Creating XLM-R embeddings\"):\n",
        "        emb = get_xlm_embedding(text)\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    embeddings_np = np.vstack(embeddings)\n",
        "\n",
        "    # Сохраняем в .npz\n",
        "    save_file = os.path.join(save_path, f\"embeddings_xlm_roberta_{name_suffix}_full.npz\")\n",
        "    np.savez_compressed(save_file, embeddings=embeddings_np, indices=indices)\n",
        "\n",
        "    print(f\"Эмбеддинги сохранены в: {save_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVnx6TLvxkLS"
      },
      "outputs": [],
      "source": [
        "create_embeddings_and_save(train, text_col=\"text_norm\", name_suffix='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA7f1b7VIODR"
      },
      "outputs": [],
      "source": [
        "create_embeddings_and_save(val, text_col=\"text_norm\", name_suffix='val')\n",
        "create_embeddings_and_save(test, text_col=\"text_norm\", name_suffix='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-KFo72dyh6i"
      },
      "outputs": [],
      "source": [
        "def load_embeddings(npz_path):\n",
        "    data = np.load(npz_path)\n",
        "    return data['embeddings'], data['indices']\n",
        "\n",
        "# Загрузка эмбеддингов\n",
        "train_emb, train_idx = load_embeddings(os.path.join(save_path, \"embeddings_xlm_roberta_train_full.npz\"))\n",
        "val_emb, val_idx = load_embeddings(os.path.join(save_path, \"embeddings_xlm_roberta_val_full.npz\"))\n",
        "test_emb, test_idx = load_embeddings(os.path.join(save_path, \"embeddings_xlm_roberta_test_full.npz\"))\n",
        "\n",
        "# Создадим DataFrame из эмбеддингов с индексами\n",
        "train_emb_df = pd.DataFrame(train_emb, index=train_idx)\n",
        "val_emb_df = pd.DataFrame(val_emb, index=val_idx)\n",
        "test_emb_df = pd.DataFrame(test_emb, index=test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c89gL1zMy-13"
      },
      "outputs": [],
      "source": [
        "# Сначала обучим PCA на тренировочных эмбеддингах\n",
        "pca = PCA(n_components=256, random_state=42)\n",
        "train_emb_pca = pca.fit_transform(train_emb_df)\n",
        "\n",
        "# Преобразуем в DataFrame с теми же индексами\n",
        "train_emb_pca_df = pd.DataFrame(train_emb_pca, index=train_emb_df.index, columns=[f'pca_emb_{i}' for i in range(256)])\n",
        "\n",
        "# Применяем тот же PCA к валидационным эмбеддингам\n",
        "val_emb_pca = pca.transform(val_emb_df)\n",
        "val_emb_pca_df = pd.DataFrame(val_emb_pca, index=val_emb_df.index, columns=[f'pca_emb_{i}' for i in range(256)])\n",
        "\n",
        "# Применяем тот же PCA к тестовым эмбеддингам\n",
        "test_emb_pca = pca.transform(test_emb_df)\n",
        "test_emb_pca_df = pd.DataFrame(test_emb_pca, index=test_emb_df.index, columns=[f'pca_emb_{i}' for i in range(256)])\n",
        "\n",
        "# Затем объединяем обратно с остальными признаками\n",
        "train_with_emb = train.join(train_emb_pca_df)\n",
        "val_with_emb = val.join(val_emb_pca_df)\n",
        "test_with_emb = test.join(test_emb_pca_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HWMScijfQE3"
      },
      "outputs": [],
      "source": [
        "train_with_emb.columns[:32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOJazPSAd7Ht"
      },
      "outputs": [],
      "source": [
        "# Убираем из признаков целевые колонки и ненужные\n",
        "drop_cols = ['labor_costs', 'labor_costs_log', 'text', 'text_norm', 'company_id']\n",
        "X_train = train_with_emb.drop(columns=drop_cols, errors='ignore')\n",
        "y_train = train_with_emb['labor_costs_log']\n",
        "\n",
        "X_val = val_with_emb.drop(columns=drop_cols, errors='ignore')\n",
        "y_val = val_with_emb['labor_costs_log']\n",
        "\n",
        "X_test = test_with_emb.drop(columns=drop_cols, errors='ignore')\n",
        "y_test = test_with_emb['labor_costs_log']\n",
        "\n",
        "cat_features = [\n",
        "   # Основные категориальные\n",
        "   'importance',\n",
        "   'project_name',\n",
        "   'department_name',\n",
        "\n",
        "   # Флаги\n",
        "   'is_subtask',\n",
        "   'is_rare_company',\n",
        "   'is_rare_project',\n",
        "   'has_urgent_words',\n",
        "   'has_complex_words',\n",
        "\n",
        "   # Комбинированные категориальные ключи\n",
        "   'company_department',\n",
        "   'project_department',\n",
        "   'company_project',\n",
        "\n",
        "   # Размерные категории\n",
        "   'company_size_category',\n",
        "   'dept_size_category',\n",
        "   'project_size_category'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtmA1ICYWgWd"
      },
      "source": [
        "## Models\n",
        "все параметры модели были подобраны с помощью GridSearchCV и дальше были использованы лучшие из них"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo1DtbCid8q5"
      },
      "source": [
        "###CatBoost\n",
        "обучалась на полном датасете"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3WUKQ2XgJKj"
      },
      "outputs": [],
      "source": [
        "# Приводим категориальные признаки к строковому типу\n",
        "for col in cat_features:\n",
        "    X_train[col] = X_train[col].astype(str)\n",
        "    X_val[col] = X_val[col].astype(str)\n",
        "    X_test[col] = X_test[col].astype(str)\n",
        "\n",
        "# Создаем пулы для Catboost\n",
        "train_pool = Pool(\n",
        "    data=X_train,\n",
        "    label=y_train,\n",
        "    cat_features=cat_features\n",
        ")\n",
        "\n",
        "val_pool = Pool(\n",
        "    data=X_val,\n",
        "    label=y_val,\n",
        "    cat_features=cat_features\n",
        ")\n",
        "\n",
        "test_pool = Pool(\n",
        "    data=X_test,\n",
        "    label=y_test,\n",
        "    cat_features=cat_features\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t2WDlg_ef5n"
      },
      "outputs": [],
      "source": [
        "model = CatBoostRegressor(\n",
        "    iterations=3000,\n",
        "    learning_rate=0.01,\n",
        "    depth=7,\n",
        "    eval_metric='RMSE',\n",
        "    l2_leaf_reg=3,\n",
        "    random_seed=42,\n",
        "    task_type='GPU',\n",
        "    early_stopping_rounds=500,\n",
        "    verbose=200,\n",
        "    od_type='Iter',\n",
        "    use_best_model=True,\n",
        ")\n",
        "\n",
        "history = model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
        "\n",
        "# Предсказания и обратное преобразование\n",
        "test_preds_log = model.predict(test_pool)\n",
        "test_preds = np.expm1(test_preds_log)\n",
        "\n",
        "mse_log = mean_squared_error(y_test, test_preds_log)\n",
        "mse = mean_squared_error(test_with_emb['labor_costs'], test_preds)\n",
        "catboost_r2 = r2_score(test_with_emb['labor_costs'], test_preds)\n",
        "\n",
        "print(f\"CatBoost R²: {catboost_r2:.4f}\")\n",
        "print(f\"CatBoost MSE (log): {mse_log:.4f}\")\n",
        "print(f\"CatBoost MSE: {mse:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(model.get_evals_result()['validation']['RMSE'], label=\"Validation RMSE\")\n",
        "plt.plot(model.get_evals_result()['learn']['RMSE'], label=\"Train RMSE\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"CatBoost Learning Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqmK3_pCtr52"
      },
      "outputs": [],
      "source": [
        "# Получаем важность признаков (по количеству использований при разбиениях)\n",
        "feature_importances = model.get_feature_importance(train_pool)\n",
        "\n",
        "# Получаем названия признаков из train_pool\n",
        "feature_names = train_pool.get_feature_names()\n",
        "\n",
        "# Создаем DataFrame для удобства\n",
        "fi_df = pd.DataFrame({'feature': feature_names[:20], 'importance': feature_importances[:20]})\n",
        "\n",
        "# Сортируем по важности\n",
        "fi_df = fi_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(fi_df)\n",
        "\n",
        "# Визуализация топ-20 признаков\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(fi_df['feature'][:20][::-1], fi_df['importance'][:20][::-1], color='skyblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance (CatBoost)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Oz2ynWeBTX"
      },
      "source": [
        "###RandomForest\n",
        "обучалась на 15к"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D74_ML5eDxC"
      },
      "outputs": [],
      "source": [
        "# Подготовим данные для sklearn\n",
        "def prepare_data_for_sklearn(X_train, X_val, X_test, cat_features):\n",
        "    X_train_encoded = X_train.copy()\n",
        "    X_val_encoded = X_val.copy()\n",
        "    X_test_encoded = X_test.copy()\n",
        "    label_encoders = {}\n",
        "\n",
        "    for col in cat_features:\n",
        "        print(f\"  Кодируем {col}: {X_train[col].nunique()} уникальных значений\")\n",
        "\n",
        "        # Создаем LabelEncoder для каждого признака\n",
        "        le = LabelEncoder()\n",
        "\n",
        "        # Обучаем на train + val для покрытия всех категорий\n",
        "        all_values = pd.concat([X_train[col], X_val[col], X_test[col]]).astype(str)\n",
        "        le.fit(all_values)\n",
        "\n",
        "        # Применяем к train и val\n",
        "        X_train_encoded[col] = le.transform(X_train[col].astype(str))\n",
        "        X_val_encoded[col] = le.transform(X_val[col].astype(str))\n",
        "        X_test_encoded[col] = le.transform(X_test[col].astype(str))\n",
        "\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    return X_train_encoded, X_val_encoded, X_test_encoded, label_encoders\n",
        "\n",
        "X_train_sklearn, X_val_sklearn, X_test_sklearn, encoders = prepare_data_for_sklearn(X_train, X_val, X_test, cat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjm_8Zwvs84S"
      },
      "outputs": [],
      "source": [
        "# Инициализация модели\n",
        "rf_model = RandomForestRegressor(\n",
        "    max_depth=15,\n",
        "    max_features='sqrt',\n",
        "    min_samples_leaf=2,\n",
        "    min_samples_split=20,\n",
        "    n_estimators=500,\n",
        "    random_state=42,\n",
        "    n_jobs=-1)\n",
        "\n",
        "\n",
        "# Обучение\n",
        "rf_model.fit(X_train_sklearn, y_train)\n",
        "\n",
        "test_preds_rf_log = rf_model.predict(X_test_sklearn)\n",
        "test_preds_rf = np.expm1(test_preds_rf_log)\n",
        "\n",
        "mse_log_rf = mean_squared_error(y_test, test_preds_rf_log)\n",
        "mse_rf = mean_squared_error(test_with_emb['labor_costs'], test_preds_rf)\n",
        "rf_r2 = r2_score(test_with_emb['labor_costs'], test_preds_rf)\n",
        "\n",
        "print(f\"RF R²: {rf_r2:.4f}\")\n",
        "print(f\"RF MSE (log): {mse_log_rf:.4f}\")\n",
        "print(f\"RF MSE: {mse_rf:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyIE8tvjeNBi"
      },
      "outputs": [],
      "source": [
        "# Feature importance для RF\n",
        "rf_importance = pd.DataFrame({\n",
        "    'feature': X_train_sklearn.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "emb_features_rf = [f for f in rf_importance['feature'] if 'pca_emb_' in f]\n",
        "emb_importance_rf = rf_importance[rf_importance['feature'].isin(emb_features_rf)]['importance'].sum()\n",
        "total_importance_rf = rf_importance['importance'].sum()\n",
        "\n",
        "print(f\"RandomForest - доля важности эмбеддингов: {emb_importance_rf/total_importance_rf:.1%}\")\n",
        "\n",
        "top_features = rf_importance.sort_values('importance', ascending=False).head(15)\n",
        "\n",
        "top_features['importance_pct'] = top_features['importance'] * 100\n",
        "print(top_features[['feature', 'importance_pct']])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_features['importance']*100, y=top_features['feature'])\n",
        "plt.xlabel('Importance (%)')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Топ 15 признаков по важности (RandomForest)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Its8okVeXOH"
      },
      "source": [
        "###LightGBM\n",
        "обучалась на полном датасете"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8PfrZIDIRyn"
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_lightgbm(X_train, X_val, X_test, cat_features):\n",
        "    X_train_lgb = X_train.copy()\n",
        "    X_val_lgb = X_val.copy()\n",
        "    X_test_lgb = X_test.copy()\n",
        "\n",
        "    for col in cat_features:\n",
        "        le = LabelEncoder()\n",
        "        all_values = pd.concat([X_train[col], X_val[col], X_test[col]]).astype(str)\n",
        "        le.fit(all_values)\n",
        "\n",
        "        X_train_lgb[col] = le.transform(X_train[col].astype(str))\n",
        "        X_val_lgb[col] = le.transform(X_val[col].astype(str))\n",
        "        X_test_lgb[col] = le.transform(X_test[col].astype(str))\n",
        "\n",
        "    return X_train_lgb, X_val_lgb, X_test_lgb\n",
        "\n",
        "X_train_lgb, X_val_lgb, X_test_lgb = prepare_data_for_lightgbm(X_train, X_val, X_test, cat_features)\n",
        "\n",
        "lgb_train = lgb.Dataset(X_train_lgb, y_train, categorical_feature=cat_features)\n",
        "lgb_val = lgb.Dataset(X_val_lgb, y_val, reference=lgb_train, categorical_feature=cat_features)\n",
        "lgb_test = lgb.Dataset(X_test_lgb, y_test, reference=lgb_train, categorical_feature=cat_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9UTWPKQOjdB"
      },
      "outputs": [],
      "source": [
        "evals_result = {}\n",
        "\n",
        "lgb_params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'num_leaves': 100,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'lambda_l1': 0.1,\n",
        "    'lambda_l2': 0.1,\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "lgb_model = lgb.train(\n",
        "    lgb_params,\n",
        "    lgb_train,\n",
        "    valid_sets=[lgb_val],\n",
        "    num_boost_round=1000,\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(100, verbose=True),\n",
        "        lgb.record_evaluation(evals_result)\n",
        "    ]\n",
        ")\n",
        "\n",
        "lgb_pred_log = lgb_model.predict(X_test_lgb)\n",
        "lgb_pred = np.expm1(lgb_pred_log)\n",
        "\n",
        "lgb_mse_log = mean_squared_error(y_test, lgb_pred_log)\n",
        "mse_lgb = mean_squared_error(test_with_emb['labor_costs'], lgb_pred)\n",
        "lgb_r2 = r2_score(test_with_emb['labor_costs'], lgb_pred)\n",
        "\n",
        "print(f\"LightGBM R²: {lgb_r2:.4f}\")\n",
        "print(f\"LightGBM MSE (log): {lgb_mse_log:.4f}\")\n",
        "print(f\"LightGBM MSE: {mse_lgb:.4f}\")\n",
        "print(f\"LightGBM лучший RMSE: {lgb_model.best_score['valid_0']['rmse']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRvJA0uC2_V2"
      },
      "outputs": [],
      "source": [
        "# Feature importance для LightGBM\n",
        "lgb_importance = pd.DataFrame({\n",
        "    'feature': X_train_lgb.columns,\n",
        "    'importance': lgb_model.feature_importance()\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "emb_importance_lgb = lgb_importance[lgb_importance['feature'].str.contains('pca_emb_')]['importance'].sum()\n",
        "total_importance_lgb = lgb_importance['importance'].sum()\n",
        "\n",
        "print(f\"LightGBM - доля важности эмбеддингов: {emb_importance_lgb/total_importance_lgb:.1%}\")\n",
        "\n",
        "top_features = lgb_importance.sort_values('importance', ascending=False).head(15)\n",
        "\n",
        "top_features['importance_pct'] = top_features['importance']\n",
        "print(top_features[['feature', 'importance_pct']])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_features['importance'], y=top_features['feature'])\n",
        "plt.xlabel('Importance (%)')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Топ 15 признаков по важности (LGB)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEtRQvYO35qX"
      },
      "source": [
        "###XGBoost\n",
        "обучалась на 15к"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpcG3GEH3OUh"
      },
      "outputs": [],
      "source": [
        "xgb_model = XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    colsample_bytree=1.0,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=5,\n",
        "    n_estimators=300,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1,\n",
        "    submsample=0.8,\n",
        "    random_state=42,\n",
        "    tree_method='hist',\n",
        "    device='cuda',\n",
        "    verbosity=1,\n",
        "    n_jobs=1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_sklearn, y_train)\n",
        "\n",
        "# Предсказания\n",
        "test_preds_log = xgb_model.predict(X_test_sklearn)\n",
        "test_preds = np.expm1(test_preds_log)\n",
        "\n",
        "mse_log = mean_squared_error(y_test, test_preds_log)\n",
        "mse_xgb = mean_squared_error(test_with_emb['labor_costs'], test_preds)\n",
        "\n",
        "print(f\"XGBoost MSE (log): {mse_log:.4f}\")\n",
        "print(f\"XGBoost MSE: {mse_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqmbe1iz4X3j"
      },
      "outputs": [],
      "source": [
        "# Feature importance для XGB\n",
        "xgb_importance = pd.DataFrame({\n",
        "    'feature': X_train_sklearn.columns,\n",
        "    'importance': xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "emb_features_rf = [f for f in xgb_importance['feature'] if 'pca_emb_' in f]\n",
        "emb_importance_rf = xgb_importance[xgb_importance['feature'].isin(emb_features_rf)]['importance'].sum()\n",
        "total_importance_rf = xgb_importance['importance'].sum()\n",
        "\n",
        "print(f\"XGBoost - доля важности эмбеддингов: {emb_importance_rf/total_importance_rf:.1%}\")\n",
        "\n",
        "top_features = xgb_importance.sort_values('importance', ascending=False).head(15)\n",
        "\n",
        "top_features['importance_pct'] = top_features['importance'] * 100\n",
        "print(top_features[['feature', 'importance_pct']])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_features['importance']*100, y=top_features['feature'])\n",
        "plt.xlabel('Importance (%)')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Топ 15 признаков по важности (XGBoost)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ETW_5GDzGF0"
      },
      "source": [
        "###LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p-tsMDA5c_8"
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_linear_models(X_train, X_val, cat_features, num_features):\n",
        "\n",
        "    # OneHot для категориальных\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "    # StandardScaler для числовых\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Трансформер\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", scaler, num_features),\n",
        "            (\"cat\", ohe, cat_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # fit + transform train\n",
        "    X_train_prepared = preprocessor.fit_transform(X_train)\n",
        "\n",
        "    # transform val\n",
        "    X_val_prepared = preprocessor.transform(X_val)\n",
        "\n",
        "    return X_train_prepared, X_val_prepared\n",
        "\n",
        "\n",
        "num_features = [col for col in X_train.columns if col not in cat_features]\n",
        "X_train_linear, X_val_linear = prepare_data_for_linear_models(X_train, X_val, cat_features, num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAjTxdPy9R-7"
      },
      "outputs": [],
      "source": [
        "param_grid_lr = {\n",
        "   'fit_intercept': [True],\n",
        "   'positive': [False, True]\n",
        "}\n",
        "\n",
        "lr_model = LinearRegression(\n",
        "   n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search_lr = GridSearchCV(\n",
        "   estimator=lr_model,\n",
        "   param_grid=param_grid_lr,\n",
        "   cv=3,\n",
        "   scoring='neg_mean_squared_error',\n",
        "   verbose=2,\n",
        "   n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search_lr.fit(X_train_linear, y_train)\n",
        "best_lr = grid_search_lr.best_estimator_\n",
        "\n",
        "# Предсказания\n",
        "val_preds_log = best_lr.predict(X_val_sklearn)\n",
        "val_preds = np.expm1(val_preds_log)\n",
        "\n",
        "mse_log = mean_squared_error(y_val, val_preds_log)\n",
        "mse = mean_squared_error(val_with_emb['labor_costs'], val_preds)\n",
        "\n",
        "print(f\"LR MSE (log): {mse_log:.4f}\")\n",
        "print(f\"LR MSE: {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrwoaLSWlf4d"
      },
      "source": [
        "###NN\n",
        "обучалась на 100к данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cJ0fFYInGIe"
      },
      "outputs": [],
      "source": [
        "LR=0.003\n",
        "EPOCHS=200\n",
        "BATCH_SIZE=1024\n",
        "PATIENCE=25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbSBwbLj_jMr"
      },
      "outputs": [],
      "source": [
        "# Подготовка данных\n",
        "def prepare_data(X_train, X_val, y_train, y_val, batch_size):\n",
        "    # Стандартизация признаков\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    # Конвертация в tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "    X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
        "    y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1))\n",
        "    y_val_tensor = torch.FloatTensor(y_val.values.reshape(-1, 1))\n",
        "\n",
        "    # Создание DataLoader'ов\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_dataloader, val_dataloader, scaler\n",
        "\n",
        "train_dataloader, val_dataloader, scaler = prepare_data(\n",
        "    X_train_sklearn, X_val_sklearn, y_train, y_val, batch_size=BATCH_SIZE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cEfTj2Gl73s"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNN(nn.Module):\n",
        "  def __init__(self, input_size, dropout_rate=0.3, max_log_value = 5):\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "\n",
        "    self.max_log_value = max_log_value\n",
        "\n",
        "    self.network = nn.Sequential(\n",
        "        # input_size -> 512\n",
        "        nn.Linear(input_size, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "\n",
        "        # 512 -> 1024\n",
        "        nn.Linear(512, 1024),\n",
        "        nn.BatchNorm1d(1024),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "\n",
        "        # 1024 -> 512\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "\n",
        "        # 512 -> 256\n",
        "        nn.Linear(512, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "\n",
        "        # 256 -> 128\n",
        "        nn.Linear(256, 128),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "\n",
        "        # 128 -> 64\n",
        "        nn.Linear(128, 64),\n",
        "        nn.BatchNorm1d(64),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout_rate / 2),\n",
        "\n",
        "        # 64 -> 1\n",
        "        nn.Linear(64, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.network(x)\n",
        "    x = torch.clamp(x, min=-2.0, max=self.max_log_value)\n",
        "\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixTRwPjpoaqZ"
      },
      "outputs": [],
      "source": [
        "# Создание модели\n",
        "input_size = X_train_sklearn.shape[1]\n",
        "print(input_size)\n",
        "model = FeedForwardNN(input_size, dropout_rate=0.3)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIzw3a1NmFg5"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.001, restore_best_weights=True, verbose=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            if self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            if self.verbose:\n",
        "                print(f\"Validation loss improved to {val_loss:.4f}\")\n",
        "        else:\n",
        "            # Нет улучшения\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"No improvement for {self.counter}/{self.patience} epochs\")\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "            if self.restore_best_weights and self.best_weights:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "                if self.verbose:\n",
        "                    print(\"Restored best weights\")\n",
        "            if self.verbose:\n",
        "                print(\"Early stopping triggered!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUd2URbwo75t"
      },
      "outputs": [],
      "source": [
        "# Loss function для регрессии\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "\n",
        "# Scheduler\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxVwRfxcmU8f"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, loss_fn, scheduler, grad_clip=1.0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    loss_history = []\n",
        "\n",
        "    for batch_idx, (input_data, labels) in enumerate(tqdm(dataloader)):\n",
        "        input_data = input_data.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_data)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, loss_history\n",
        "\n",
        "def validate(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_data, labels in tqdm(dataloader):\n",
        "            input_data = input_data.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_data)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcL0eelgmeqX"
      },
      "outputs": [],
      "source": [
        "# История обучения\n",
        "losses = {'train': [], 'val': []}\n",
        "batch_losses_list = []\n",
        "best_val_loss = float('inf')\n",
        "early_stopping = EarlyStopping(patience=PATIENCE, min_delta=0.001, verbose=True)\n",
        "best_model_path = None\n",
        "\n",
        "# Основной цикл обучения\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEPOCH {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "    train_loss, train_loss_history = train_one_epoch(\n",
        "        model, train_dataloader, optimizer, loss_fn, scheduler, grad_clip=1.0\n",
        "    )\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    val_loss = validate(model, val_dataloader, loss_fn)\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Сохранение лучшей модели (по val_loss для регрессии)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        # best_model_path = \"some_local_path.pth\"\n",
        "        # torch.save({\n",
        "        #     'model_state_dict': model.state_dict(),\n",
        "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
        "        #     'scheduler_state_dict': scheduler.state_dict(),\n",
        "        #     'epoch': epoch + 1,\n",
        "        #     'input_size': input_size,\n",
        "        #     'dropout_rate': 0.3,\n",
        "        #     'train_loss': train_loss,\n",
        "        #     'val_loss': val_loss,\n",
        "        #     'best_val_loss': best_val_loss,\n",
        "        # }, best_model_path)\n",
        "        print(f\"Saved new best model! Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    losses['train'].append(train_loss)\n",
        "    losses['val'].append(val_loss)\n",
        "    batch_losses_list.extend(train_loss_history)\n",
        "\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "        print(f\"Best validation loss: {early_stopping.best_loss:.4f}\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kxuuCPPiu8o"
      },
      "outputs": [],
      "source": [
        "# # загрузка лучшей модели\n",
        "# print(f\"Загружаем лучшую модель из: {best_model_path}\")\n",
        "# checkpoint = torch.load(best_model_path, map_location=device)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# print(f\"Лучшая модель загружена! Epoch: {checkpoint['epoch']}, Val Loss: {checkpoint['val_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwDsfUxWpQTs"
      },
      "outputs": [],
      "source": [
        "# Финальные предсказания\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_data, _ in val_dataloader:\n",
        "        input_data = input_data.to(device)\n",
        "        outputs = model(input_data)\n",
        "        predictions.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "val_preds_log = np.array(predictions)\n",
        "val_preds = np.expm1(val_preds_log)\n",
        "\n",
        "# Метрики\n",
        "mse_log = mean_squared_error(y_val, val_preds_log)\n",
        "mse_nn = mean_squared_error(val_with_emb['labor_costs'], val_preds)\n",
        "\n",
        "print(f\"\\nФинальные результаты:\")\n",
        "print(f\"Neural Network MSE (log): {mse_log:.4f}\")\n",
        "print(f\"Neural Network MSE: {mse_nn:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ijURNw1pcxE"
      },
      "outputs": [],
      "source": [
        "# Построение графиков обучения\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(batch_losses_list, label=\"Batch Loss\")\n",
        "plt.xlabel(\"Batch iteration\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training Loss per Batch\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(val_with_emb['labor_costs'], val_preds, alpha=0.5)\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Predictions vs Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqrfk3pjsO9p"
      },
      "outputs": [],
      "source": [
        "mses = {}\n",
        "mses['Catboost'] = mse\n",
        "mses['RandomForest'] = mse_rf\n",
        "mses['LightGBM'] = mse_lgb\n",
        "mses['XGBoost'] = mse_xgb\n",
        "\n",
        "mses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AjIV5PtYAyj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(mses.keys(), mses.values())\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"Сравнение MSE разных моделей\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}